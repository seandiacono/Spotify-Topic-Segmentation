{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmented(path):\n",
    "    # Load segmented text\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        # Split text by delimiter\n",
    "        text = text.split('\\n==========\\n')\n",
    "        # Strip whitespace\n",
    "        text = [x.strip() for x in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(segmented_text, n_topics=1):\n",
    "    cv = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)\n",
    "    counts = cv.fit_transform([segmented[3]])\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_tf = tfidf.fit_transform(counts)\n",
    "\n",
    "    components = n_topics\n",
    "    lda = LDA(n_components=components)\n",
    "    lda_array = lda.fit_transform(x_tf)\n",
    "\n",
    "    components = [lda.components_[i] for i in range(len(lda.components_))]\n",
    "    features = cv.get_feature_names()\n",
    "    important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:5] for j in range(len(components))]\n",
    "    print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = load_segmented('../text_segmentation/transcripts_tiling.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But it was almost a good thing. I don't know. I feel like it almost made us realize how good our friendship was because we'd spend that time with him. What what where do ya were meant to be actually in contact with each other and why don't we take so long to rekindle out again? And then yeah, I'm pretty much just said to me next time you're in Perth if you have a job like dude come stay with me and I was like,\n",
      "\n",
      " I will actually take you up on the offer and I did and it was like nothing changed and it was like exactly the same as what we picked up where we left off. So, yeah, and it's really anything. I feel like we had a before years apart or five or whatever you want to say is yeah, we definitely needed it might we were both we both spend some time figuring out some things trying to figure out who has corny as it sounds who we were letting myself. Sorry. And yeah, and then yeah meeting back together. I feel like the Friendship we had was a more meaningful and sincere friendship than what we had before tones. Like we were, you know, we were friends before but now we're like we were friends were friends now. It's like honestly, it's actually on another level and I feel like yeah, we've gone through a lot together. Mmm. We had we have had a lot of me. A lot of mems a lot of men's let's have a think. All right. I am Inc. We're going to the Mims guys. All right, I remember back in storytime. You five Gulf War. You shaved your head. Oh, I got a good story about me. Oh my goodness guys. Oh, I don't know if you know this you might be finding out of Revelation is I'm saying\n",
      "\n",
      " it's nothing too bad that you're comparing excited guys. Okay, so I shaved my head because my granddad died of well, it did lead out of cancer. So it was for a good reason, but sorry that I have so much Christian Christian did it to she shaved her head and so did I think Jackson did to my husband he did it too because he said he shaved his head because he told me that he did. Schools well, and I think there was maybe one other person really well, but yeah, so it was like four of us in our class who did it. But yeah, it was hectic times trying to off that what a freaking do this. It was I mean, of course for good reason, but when you're a kid, yes six you get I was so self-conscious. So super self-conscious of having that canteen my had a bandana on all the time and so to Christian and so did\n",
      "\n",
      " A few other people, but I just remember they were in sorry. We're like name-dropping right now, but I'm just yeah, everyone loved those. Yeah, like we love them pretty sure mom never gave me three dollars. Yeah I want and then I couldn't bother wearing anymore because my head got a little bit and I'm just moving my going through her head. Jeff spy can do it was so mad hot. I have to tell the story is the funniest thing so ou6, right and I'll teach at the time. Think that I'm just thinking about it maybe gonna cream. So I God so de jogos to write something that was nice about each other like places like give someone a compliment and she gave everyone a person to say one good thing. Yeah the other hand I think I got one of the boys, of course to give me a compliment quotation mark compliment. And what does the bloody kid, right I won't say who it was. I know who it was, but it was so all that said I read are open up my little piece of paper as a self-conscious year 6 and what does it say Ruth is so good at being bald. Like Sosa and defended I don't think I cried but I almost did but so and like so embarrassment us just like I hate my life right now is like funny Kim write this so mean and I win over to me.\n"
     ]
    }
   ],
   "source": [
    "print(segmented[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  3  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  3  1  1  1  3  3  1\n",
      "   1  2  1  1  1  1  1  1  1  1  1  1  2  1  1  4  1  1  1  2  1  1  3  3\n",
      "   1  1  2  1  2  1  2  1  1  7  1  3  1  1  3  1  1  6  1  1  1  1  1  1\n",
      "   1  1  1  2  1  5  1  1  1  1  1  1 20  2  1  4  1  1  1  1  1  2  2  1\n",
      "   1  1  1  1  2  1  1  1  1  2  1  1  1  1  1  2  1  3  1  3  1  1  1  1\n",
      "   2  1  1  1  1  2  1  1  1  2  2  1  2  1  6  1  3  4  1  1  3  4  1  1\n",
      "   1  1  3  1  1  2  1  1  2  1  1  1  2  1  1  3  1  6  1  1  5  1  2  1\n",
      "   1  2  3  2  1  1  3  1  1  2  1 12  1  1  1]]\n",
      "[[0.07999529 0.92000471]]\n",
      "[['like', 'yeah', 'good', 'head', 'right'], ['like', 'yeah', 'good', 'think', 'head']]\n"
     ]
    }
   ],
   "source": [
    "topic_modelling(segmented, n_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = load_segmented('../text_segmentation/sentence_segmentation.txt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "991933e55c9ad2a33e133c9a66de8b6c9f625dc5b9c7ac9b8ec31bdc5c541ed9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('thesisenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
