{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segmented(path):\n",
    "    # Load segmented text\n",
    "    with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "        # Split text by delimiter\n",
    "        text = text.split('\\n==========\\n')\n",
    "        # Strip whitespace\n",
    "        text = [x.strip() for x in text]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(segmented_text, n_topics=1):\n",
    "    cv = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)\n",
    "    counts = cv.fit_transform([segmented[3]])\n",
    "\n",
    "    tfidf = TfidfTransformer()\n",
    "    x_tf = tfidf.fit_transform(counts)\n",
    "\n",
    "    components = n_topics\n",
    "    lda = LDA(n_components=components)\n",
    "    lda_array = lda.fit_transform(x_tf)\n",
    "\n",
    "    components = [lda.components_[i] for i in range(len(lda.components_))]\n",
    "    features = cv.get_feature_names()\n",
    "    important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:5] for j in range(len(components))]\n",
    "    print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = load_segmented('../text_segmentation/transcripts_tiling.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What about the misses the what's he want to say teacher? And do you want to go? No, you see that was the letter thing?\n",
      "\n",
      " I don't remember as much of that because I'm your marriage thing for me was so embarrassed. Yeah. I know. It's look I was I will admit. I was somewhat the teacher's pet back in primary school and I look back and I cringe, you know, like I was\n"
     ]
    }
   ],
   "source": [
    "print(segmented[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'desk', 'good', 'time', 'actually'], ['like', 'business', 'make', 'actually', 'time']]\n"
     ]
    }
   ],
   "source": [
    "topic_modelling(segmented, n_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = load_segmented('../text_segmentation/sentence_segmentation.txt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "991933e55c9ad2a33e133c9a66de8b6c9f625dc5b9c7ac9b8ec31bdc5c541ed9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('thesisenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
